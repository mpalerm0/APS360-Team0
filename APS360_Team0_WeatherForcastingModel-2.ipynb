{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Team 0: Weather Forecasting Model with RNN\n",
        "\n",
        "##Members\n",
        "*   Matthew Grech\n",
        "*   Andrew Radke\n",
        "*   Liza Abraham\n",
        "*   Mitchell Palermo\n",
        "\n",
        "####Created: Jun 15, 2023\n",
        "This model will take in weather data and return a prediction for the amount of rainfall in a day in mm\n",
        "\n"
      ],
      "metadata": {
        "id": "T034fHXZ5ByG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Import Libraries and Mount Google Drive to Load Data"
      ],
      "metadata": {
        "id": "sD_GWrcqzqf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import statsmodels.api as sm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from statsmodels.tsa.statespace.varmax import VARMAX\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "E93Vzisizo7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfNBkSbVtED7",
        "outputId": "e2dbf761-cfcd-4a65-8964-53dadd8eec4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Define The Nerual Network as a LSTM RNN"
      ],
      "metadata": {
        "id": "MnOykq5z1EFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers = 10)\n",
        "        self.dense = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "\n",
        "        # Take the last hidden state h_n as output\n",
        "        out = self.dense(h_n[-1])\n",
        "        return out"
      ],
      "metadata": {
        "id": "b0mWx3Nx1Och"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEMP: Daily Data for Testing\n",
        "\n",
        "should replace step 3"
      ],
      "metadata": {
        "id": "YNn7feXxgFiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "hxAnb6OugQx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "uRZO0abOgY9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
        "from pyspark.sql.functions import col, when"
      ],
      "metadata": {
        "id": "DMKxR56agjAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a spark session using getOrCreate() function\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Applying custom schema to data frame\n",
        "df = spark.read.format(\n",
        "    \"csv\").option(\n",
        "    \"header\", True).load(\"/content/drive/MyDrive/Dataset2\") #Matt's path /content/drive/MyDrive/Dataset2\n",
        "# Display the updated schema of the data frame\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "Uqk7f84Agjd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c743c7-e16b-4ff6-f7c5-0a706c96b171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Longitude (x): string (nullable = true)\n",
            " |-- Latitude (y): string (nullable = true)\n",
            " |-- Station Name: string (nullable = true)\n",
            " |-- Climate ID: string (nullable = true)\n",
            " |-- Date/Time (LST): string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- Day: string (nullable = true)\n",
            " |-- Time (LST): string (nullable = true)\n",
            " |-- Temp (°C): string (nullable = true)\n",
            " |-- Temp Flag: string (nullable = true)\n",
            " |-- Dew Point Temp (°C): string (nullable = true)\n",
            " |-- Dew Point Temp Flag: string (nullable = true)\n",
            " |-- Rel Hum (%): string (nullable = true)\n",
            " |-- Rel Hum Flag: string (nullable = true)\n",
            " |-- Precip. Amount (mm): string (nullable = true)\n",
            " |-- Precip. Amount Flag: string (nullable = true)\n",
            " |-- Wind Dir (10s deg): string (nullable = true)\n",
            " |-- Wind Dir Flag: string (nullable = true)\n",
            " |-- Wind Spd (km/h): string (nullable = true)\n",
            " |-- Wind Spd Flag: string (nullable = true)\n",
            " |-- Visibility (km): string (nullable = true)\n",
            " |-- Visibility Flag: string (nullable = true)\n",
            " |-- Stn Press (kPa): string (nullable = true)\n",
            " |-- Stn Press Flag: string (nullable = true)\n",
            " |-- Hmdx: string (nullable = true)\n",
            " |-- Hmdx Flag: string (nullable = true)\n",
            " |-- Wind Chill: string (nullable = true)\n",
            " |-- Wind Chill Flag: string (nullable = true)\n",
            " |-- Weather: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Year\", \"Month\", \"Day\", \"Time (LST)\", \"Dew Point Temp (°C)\", \"Rel Hum (%)\", \"Wind Spd (km/h)\", \"Visibility (km)\", \"Stn Press (kPa)\", \"Temp (°C)\"]\n",
        "df = df.select(*columns)"
      ],
      "metadata": {
        "id": "XhQYjIvTerxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_mapping={\"Year\": \"year\", \"Month\": \"month\", \"Day\": \"day\", \"Time (LST)\": \"time\", \"Dew Point Temp (°C)\": \"dew_point_temp\", \"Rel Hum (%)\": \"rem_hum\", \"Wind Spd (km/h)\": \"wind_speed\",\n",
        "                \"Visibility (km)\": \"visibility\", \"Stn Press (kPa)\": \"press\", \"Temp (°C)\": \"temp\"}\n",
        "for old_col, new_col in column_mapping.items():\n",
        "    df = df.withColumnRenamed(old_col, new_col)"
      ],
      "metadata": {
        "id": "2SQYoFUie1gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_cast = [\n",
        "    ('year', 'int'),\n",
        "    ('month', 'int'),\n",
        "    ('day', 'int'),\n",
        "    ('time', 'string'),\n",
        "    ('dew_point_temp', 'double'),\n",
        "    ('rem_hum', 'int'),\n",
        "    ('wind_speed', 'int'),\n",
        "    ('visibility', 'double'),\n",
        "    ('press', 'double'),\n",
        "    ('temp', 'double')\n",
        "]\n",
        "\n",
        "# Cast the columns to the specified data types\n",
        "for col_name, data_type in columns_to_cast:\n",
        "    df = df.withColumn(col_name, col(col_name).cast(data_type))"
      ],
      "metadata": {
        "id": "_NeAF8vve41U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.orderBy([\"year\", \"month\", \"day\", \"time\"])\n",
        "df = df.drop(\"year\", \"month\", \"day\", \"time\")"
      ],
      "metadata": {
        "id": "Ya6SUIExe5e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################## IGNORE ########################################\n",
        "################################# for daily ####################################\n",
        "# columns = [\"Longitude (x)\", \"Latitude (y)\", \"Station Name\", \"Climate ID\", \"Date/Time\", \"Year\", \"Month\", \"Day\", \"Max Temp (°C)\",\n",
        "#          \"Min Temp (°C)\", \"Mean Temp (°C)\", \"Total Rain (mm)\", \"Total Snow (cm)\", \"Total Precip (mm)\", \"Spd of Max Gust (km/h)\"]\n",
        "# df = df.select(*columns)\n",
        "# column_mapping={\"Longitude (x)\": \"long\", \"Latitude (y)\": \"lat\", \"Station Name\": \"location_name\", \"Climate ID\": \"location_id\",\n",
        "#              \"Date/Time\": \"date\", \"Year\": \"year\", \"Month\": \"month\", \"Day\": \"day\", \"Max Temp (°C)\": \"max_temp\",\n",
        "#          \"Min Temp (°C)\": \"min_temp\", \"Mean Temp (°C)\": \"avg_temp\", \"Total Rain (mm)\": \"total_rain\", \"Total Snow (cm)\": \"total_snow\",\n",
        "#              \"Total Precip (mm)\": \"total_percip\", \"Spd of Max Gust (km/h)\": \"wind_speed\"}\n",
        "# for old_col, new_col in column_mapping.items():\n",
        "#     df = df.withColumnRenamed(old_col, new_col)\n",
        "\n",
        "# columns_to_cast = [\n",
        "#     ('long', 'double'),\n",
        "#     ('lat', 'double'),\n",
        "#     ('location_name', 'string'),\n",
        "#     ('location_id', 'int'),\n",
        "#     ('date', 'string'),\n",
        "#     ('year', 'int'),\n",
        "#     ('month', 'int'),\n",
        "#     ('day', 'int'),\n",
        "#     ('max_temp', 'double'),\n",
        "#     ('min_temp', 'double'),\n",
        "#     ('avg_temp', 'double'),\n",
        "#     ('total_rain', 'double'),\n",
        "#     ('total_snow', 'double'),\n",
        "#     ('total_percip', 'double')\n",
        "# ]\n",
        "\n",
        "# # Cast the columns to the specified data types\n",
        "# for col_name, data_type in columns_to_cast:\n",
        "#     df = df.withColumn(col_name, col(col_name).cast(data_type))\n",
        "\n",
        "# df = df.withColumn(\"is_less_than_31\", when(col(\"wind_speed\") == \"<31\", True).otherwise(False))\n",
        "# df = df.withColumn(\"wind_speed\", when(col(\"wind_speed\") == \"<31\", None).otherwise(col(\"wind_speed\")))\n",
        "# df = df.withColumn(\"wind_speed\", col(\"wind_speed\").cast(\"int\"))"
      ],
      "metadata": {
        "id": "KovDq41OgouY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.toPandas()"
      ],
      "metadata": {
        "id": "gtbJujS-hu3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfTensor = torch.tensor(df.values, dtype=torch.float32)\n",
        "dfTensor = dfTensor.unsqueeze(0)\n",
        "\n",
        "# # Reshape the tensor to include the sequence length dimension\n",
        "# seq_len = 3\n",
        "# batch_size = 8768\n",
        "# dfTensor = dfTensor.view(batch_size, seq_len, dfTensor.shape[1])\n",
        "\n",
        "# Create a TensorDataset from the reshaped tensor\n",
        "trainingDataset = TensorDataset(dfTensor)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainingDataset, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkytXPmHtN5e",
        "outputId": "08e0fe89-f2e6-4a34-aa9f-54a586d11323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(-4.1000),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the dtype attribute\n",
        "result = df.dtypes\n",
        "print(df)\n",
        "print(dfTensor)\n",
        "# print(\"Output:\")\n",
        "# print(result)\n",
        "# print(\"TrainLoader\")\n",
        "\n",
        "# print(\"Input batch:\")\n",
        "# print(inputs_batch)\n",
        "\n",
        "# print(\"Target batch:\")\n",
        "# print(targets_batch)\n",
        "# print(\"Something else batch:\")\n",
        "# print(something_else)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uBb43r_LWYB",
        "outputId": "2c2c4ed9-410e-4d52-a3e3-fada13796108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       dew_point_temp  rem_hum  wind_speed  visibility  press  temp\n",
            "0                -4.1     77.0        14.0        11.3  99.32  -0.6\n",
            "1                -4.8     72.0        17.0        19.3  99.29  -0.4\n",
            "2                -4.6     74.0        14.0        19.3  99.23  -0.5\n",
            "3                -4.8     73.0        20.0        19.3  99.22  -0.5\n",
            "4                -4.2     77.0        21.0        19.3  99.19  -0.7\n",
            "...               ...      ...         ...         ...    ...   ...\n",
            "26299             1.2     96.0        16.0         6.4  98.28   1.8\n",
            "26300             1.7     97.0        18.0         3.6  97.95   2.2\n",
            "26301             2.6     97.0        18.0         3.2  97.79   3.0\n",
            "26302             2.9     98.0        24.0         2.8  97.34   3.2\n",
            "26303             3.2     98.0        21.0         4.8  97.13   3.5\n",
            "\n",
            "[26304 rows x 6 columns]\n",
            "tensor([[-4.1000, 77.0000, 14.0000, 11.3000, 99.3200, -0.6000],\n",
            "        [-4.8000, 72.0000, 17.0000, 19.3000, 99.2900, -0.4000],\n",
            "        [-4.6000, 74.0000, 14.0000, 19.3000, 99.2300, -0.5000],\n",
            "        ...,\n",
            "        [ 2.6000, 97.0000, 18.0000,  3.2000, 97.7900,  3.0000],\n",
            "        [ 2.9000, 98.0000, 24.0000,  2.8000, 97.3400,  3.2000],\n",
            "        [ 3.2000, 98.0000, 21.0000,  4.8000, 97.1300,  3.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3. Load and Process Data (DONT RUN for now)\n"
      ],
      "metadata": {
        "id": "z_ZU6Eu62RH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the Data\n",
        "# train_data = df.sample(frac=0.6, random_state=42)\n",
        "# valid_data = df.drop(train_data.index).sample(frac=0.5, random_state=42)\n",
        "# test_data = df.drop(train_data.index).drop(valid_data.index)\n",
        "\n",
        "# Define a function for preprocessing the data and creating input sequences\n",
        "def create_input_sequences(data, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        sequence = data[i:i + sequence_length, :-1] #get sequence_length number of days of input data\n",
        "        target = data[i + sequence_length, -1] # get the coresponding percipitation values\n",
        "        sequences.append(sequence)\n",
        "        targets.append(target)\n",
        "    return sequences, targets\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "sequence_length = 5  # Number of previous time steps to consider\n",
        "batch_size = 32\n",
        "\n",
        "# Create input sequences and targets for training, validation and testing data\n",
        "train_sequences, train_targets = create_input_sequences(train_data.values, sequence_length)\n",
        "valid_sequences, valid_targets = create_input_sequences(valid_data.values, sequence_length)\n",
        "test_sequences, test_targets = create_input_sequences(test_data.values, sequence_length)\n",
        "\n",
        "# Convert the lists to PyTorch tensors\n",
        "train_sequences = torch.tensor(train_sequences, dtype=torch.float32)\n",
        "train_targets = torch.tensor(train_targets, dtype=torch.float32)\n",
        "\n",
        "valid_sequences = torch.tensor(valid_sequences, dtype=torch.float32)\n",
        "valid_targets = torch.tensor(valid_targets, dtype=torch.float32)\n",
        "\n",
        "test_sequences = torch.tensor(test_sequences, dtype=torch.float32)\n",
        "test_targets = torch.tensor(test_targets, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoaders's for training, validation and testing data\n",
        "train_dataset = TensorDataset(train_sequences, train_targets)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset = TensorDataset(valid_sequences, valid_targets)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_dataset = TensorDataset(test_sequences, test_targets)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "pqWAoOUJ2Qre",
        "outputId": "75c4e0b5-8708-48a1-ce5d-9bbeef3e7339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-27e8e5b4a88b>:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_sequences = torch.tensor(train_sequences, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-27e8e5b4a88b>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Convert the lists to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrain_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mtrain_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: must be real number, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#?Baseline Model:"
      ],
      "metadata": {
        "id": "2Fa7bWr8OwQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.statespace.varmax import VARMAX\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "train = df.iloc[1:100, :-1]\n",
        "\n",
        "def VARMAX_model(train, test):\n",
        "    # Fit model\n",
        "    model = VARMAX(train.iloc[1:1000], order=(1, 1))\n",
        "    model_fit = model.fit(disp=False)\n",
        "\n",
        "    # Initialize the prediction list\n",
        "    predictions = []\n",
        "\n",
        "    # Make predictions for each time step\n",
        "    for i in range(len(test)):\n",
        "        # Get the current test input\n",
        "        exog_test = test.iloc[i, :-1].values.reshape(1, -1)\n",
        "\n",
        "        # Make prediction for the next time step\n",
        "        yhat = model_fit.forecast(steps=1, exog=exog_test)\n",
        "\n",
        "        # Extract the predicted value\n",
        "        pred_value = yhat.iloc[0, -1]\n",
        "\n",
        "        # Store the predicted value\n",
        "        predictions.append(pred_value)\n",
        "\n",
        "        # Update the model with the current test input\n",
        "        model = VARMAX(pd.concat([train, test.iloc[:i+1]]), order=(1, 1))\n",
        "        model_fit = model.fit(disp=False)\n",
        "\n",
        "    # Create a DataFrame with the predicted values and the corresponding actual values\n",
        "    res = pd.DataFrame({'Pred': predictions, 'Act': test.iloc[:, -1].values})\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse = mean_squared_error(res['Act'], res['Pred'])\n",
        "    print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "    # Plot the actual and predicted data points\n",
        "    plt.plot(res['Act'], label='Actual')\n",
        "    plt.plot(res['Pred'], label='Predicted')\n",
        "    plt.xlabel('Test Number')\n",
        "    plt.ylabel('Temperature')\n",
        "    plt.title('VARMAX Model - Actual vs Predicted')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return res\n",
        "\n",
        "# Example usage:\n",
        "# train = df(100, 7)\n",
        "print(df.shape)\n",
        "train = df[1:1000]\n",
        "test = df[1001: 1009]\n",
        "\n",
        "df_ret = VARMAX_model(train, test)\n",
        "print(df_ret)\n",
        "\n"
      ],
      "metadata": {
        "id": "HY5zJ0GXXv8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4. Define Hyperparameters and Model Settings"
      ],
      "metadata": {
        "id": "LGOS0Oij2RoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "input_size = 6\n",
        "hidden_size = 120\n",
        "output_size = 1\n",
        "num_epochs = 60\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, output_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#############################################################\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "\n",
        "# model = keras.sequential()\n",
        "# model.add(layers.LSTM(120, dropout=.1,recurrent_dropout=.2, activation='relu', return_sequences=True ,input_shape=(input_size,hidden_size)))\n",
        "# model.add(layers.LSTM(120,activation='relu'))\n",
        "# model.add(layers.Dense(output_size))\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        "# print(model.summary)\n",
        "#############################################################\n"
      ],
      "metadata": {
        "id": "IC6K5G9K2RaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5. Train Model\n"
      ],
      "metadata": {
        "id": "SOO-nsSd4ZW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the input and target tensors\n",
        "dfTensor = dfTensor.squeeze(0)\n",
        "inputs = dfTensor[:-1, :]\n",
        "targets = dfTensor[1:, -1].unsqueeze(1)  # Selecting the \"temp\" column of the next row as the target"
      ],
      "metadata": {
        "id": "Y5ic1q3LR9Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dfTensor)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5m3zv9APxWG",
        "outputId": "70e37f2a-8c8d-429d-af76-847e25594d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.1000, 77.0000, 14.0000, 11.3000, 99.3200, -0.6000],\n",
            "        [-4.8000, 72.0000, 17.0000, 19.3000, 99.2900, -0.4000],\n",
            "        [-4.6000, 74.0000, 14.0000, 19.3000, 99.2300, -0.5000],\n",
            "        ...,\n",
            "        [ 2.6000, 97.0000, 18.0000,  3.2000, 97.7900,  3.0000],\n",
            "        [ 2.9000, 98.0000, 24.0000,  2.8000, 97.3400,  3.2000],\n",
            "        [ 3.2000, 98.0000, 21.0000,  4.8000, 97.1300,  3.5000]])\n",
            "tensor([[-4.1000, 77.0000, 14.0000, 11.3000, 99.3200, -0.6000],\n",
            "        [-4.8000, 72.0000, 17.0000, 19.3000, 99.2900, -0.4000],\n",
            "        [-4.6000, 74.0000, 14.0000, 19.3000, 99.2300, -0.5000],\n",
            "        ...,\n",
            "        [ 1.7000, 97.0000, 18.0000,  3.6000, 97.9500,  2.2000],\n",
            "        [ 2.6000, 97.0000, 18.0000,  3.2000, 97.7900,  3.0000],\n",
            "        [ 2.9000, 98.0000, 24.0000,  2.8000, 97.3400,  3.2000]])\n",
            "tensor([[-0.4000],\n",
            "        [-0.5000],\n",
            "        [-0.5000],\n",
            "        ...,\n",
            "        [ 3.0000],\n",
            "        [ 3.2000],\n",
            "        [ 3.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Learning rate adjustment\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Set device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Normalize the input data\n",
        "mean = inputs.mean(dim=0)\n",
        "std = inputs.std(dim=0)\n",
        "inputs = (inputs - mean) / std\n",
        "\n",
        "\n",
        "# Create the training dataset and data loader\n",
        "trainingDataset = TensorDataset(inputs, targets)\n",
        "train_loader = torch.utils.data.DataLoader(trainingDataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (inputs_batch, targets_batch) in enumerate(train_loader, 0):\n",
        "        inputs_batch = inputs_batch.to(device)\n",
        "        targets_batch = targets_batch.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs_batch)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, targets_batch.squeeze())\n",
        "\n",
        "        # Check for NaN or infinite loss values\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(f\"Invalid loss value at Epoch {epoch+1}, Batch {i+1}. Skipping batch...\")\n",
        "            continue\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    average_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.6f}\")\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step()\n",
        "    losses.append(average_loss)\n",
        "\n",
        "print(\"Training finished!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot the training loss graph\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gd1KSsHz4Y5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04877e46-02aa-4868-d734-c19722c1cb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid loss value at Epoch 1, Batch 1. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 2. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 3. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 4. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 5. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 6. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 7. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 8. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 9. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 10. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 11. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 12. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 13. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 14. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 15. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 16. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 17. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 18. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 19. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 20. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 21. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 22. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 23. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 24. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 25. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 26. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 27. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 28. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 29. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 30. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 31. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 32. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 33. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 34. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 35. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 36. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 37. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 38. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 39. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 40. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 41. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 42. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 43. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 44. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 45. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 46. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 47. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 48. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 49. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 50. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 51. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 52. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 53. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 54. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 55. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 56. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 57. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 58. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 59. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 60. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 61. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 62. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 63. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 64. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 65. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 66. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 67. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 68. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 69. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 70. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 71. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 72. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 73. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 74. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 75. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 76. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 77. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 78. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 79. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 80. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 81. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 82. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 83. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 84. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 85. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 86. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 87. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 88. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 89. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 90. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 91. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 92. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 93. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 94. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 95. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 96. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 97. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 98. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 99. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 100. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 101. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 102. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 103. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 104. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 105. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 106. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 107. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 108. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 109. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 110. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 111. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 112. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 113. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 114. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 115. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 116. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 117. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 118. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 119. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 120. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 121. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 122. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 123. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 124. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 125. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 126. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 127. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 128. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 129. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 130. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 131. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 132. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 133. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 134. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 135. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 136. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 137. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 138. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 139. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 140. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 141. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 142. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 143. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 144. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 145. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 146. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 147. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 148. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 149. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 150. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 151. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 152. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 153. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 154. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 155. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 156. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 157. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 158. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 159. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 160. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 161. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 162. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 163. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 164. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 165. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 166. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 167. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 168. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 169. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 170. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 171. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 172. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 173. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 174. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 175. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 176. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 177. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 178. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 179. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 180. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 181. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 182. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 183. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 184. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 185. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 186. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 187. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 188. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 189. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 190. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 191. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 192. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 193. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 194. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 195. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 196. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 197. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 198. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 199. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 200. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 201. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 202. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 203. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 204. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 205. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 206. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 207. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 208. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 209. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 210. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 211. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 212. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 213. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 214. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 215. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 216. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 217. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 218. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 219. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 220. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 221. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 222. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 223. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 224. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 225. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 226. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 227. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 228. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 229. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 230. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 231. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 232. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 233. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 234. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 235. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 236. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 237. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 238. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 239. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 240. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 241. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 242. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 243. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 244. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 245. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 246. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 247. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 248. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 249. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 250. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 251. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 252. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 253. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 254. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 255. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 256. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 257. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 258. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 259. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 260. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 261. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 262. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 263. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 264. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 265. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 266. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 267. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 268. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 269. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 270. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 271. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 272. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 273. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 274. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 275. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 276. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 277. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 278. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 279. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 280. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 281. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 282. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 283. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 284. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 285. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 286. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 287. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 288. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 289. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 290. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 291. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 292. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 293. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 294. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 295. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 296. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 297. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 298. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 299. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 300. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 301. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 302. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 303. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 304. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 305. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 306. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 307. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 308. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 309. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 310. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 311. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 312. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 313. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 314. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 315. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 316. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 317. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 318. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 319. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 320. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 321. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 322. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 323. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 324. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 325. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 326. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 327. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 328. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 329. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 330. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 331. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 332. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 333. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 334. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 335. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 336. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 337. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 338. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 339. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 340. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 341. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 342. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 343. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 344. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 345. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 346. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 347. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 348. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 349. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 350. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 351. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 352. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 353. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 354. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 355. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 356. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 357. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 358. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 359. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 360. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 361. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 362. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 363. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 364. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 365. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 366. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 367. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 368. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 369. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 370. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 371. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 372. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 373. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 374. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 375. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 376. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 377. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 378. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 379. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 380. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 381. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 382. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 383. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 384. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 385. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 386. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 387. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 388. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 389. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 390. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 391. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 392. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 393. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 394. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 395. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 396. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 397. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 398. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 399. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 400. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 401. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 402. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 403. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 404. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 405. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 406. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 407. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 408. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 409. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 410. Skipping batch...\n",
            "Invalid loss value at Epoch 1, Batch 411. Skipping batch...\n",
            "Epoch [1/60], Loss: 0.000000\n",
            "Invalid loss value at Epoch 2, Batch 1. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 2. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 3. Skipping batch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([63])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid loss value at Epoch 2, Batch 4. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 5. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 6. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 7. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 8. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 9. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 10. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 11. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 12. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 13. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 14. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 15. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 16. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 17. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 18. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 19. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 20. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 21. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 22. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 23. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 24. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 25. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 26. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 27. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 28. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 29. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 30. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 31. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 32. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 33. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 34. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 35. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 36. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 37. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 38. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 39. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 40. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 41. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 42. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 43. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 44. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 45. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 46. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 47. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 48. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 49. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 50. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 51. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 52. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 53. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 54. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 55. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 56. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 57. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 58. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 59. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 60. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 61. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 62. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 63. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 64. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 65. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 66. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 67. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 68. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 69. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 70. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 71. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 72. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 73. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 74. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 75. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 76. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 77. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 78. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 79. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 80. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 81. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 82. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 83. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 84. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 85. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 86. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 87. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 88. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 89. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 90. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 91. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 92. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 93. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 94. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 95. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 96. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 97. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 98. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 99. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 100. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 101. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 102. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 103. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 104. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 105. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 106. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 107. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 108. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 109. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 110. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 111. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 112. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 113. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 114. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 115. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 116. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 117. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 118. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 119. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 120. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 121. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 122. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 123. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 124. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 125. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 126. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 127. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 128. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 129. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 130. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 131. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 132. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 133. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 134. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 135. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 136. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 137. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 138. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 139. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 140. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 141. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 142. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 143. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 144. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 145. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 146. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 147. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 148. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 149. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 150. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 151. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 152. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 153. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 154. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 155. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 156. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 157. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 158. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 159. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 160. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 161. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 162. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 163. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 164. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 165. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 166. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 167. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 168. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 169. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 170. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 171. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 172. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 173. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 174. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 175. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 176. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 177. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 178. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 179. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 180. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 181. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 182. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 183. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 184. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 185. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 186. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 187. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 188. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 189. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 190. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 191. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 192. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 193. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 194. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 195. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 196. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 197. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 198. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 199. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 200. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 201. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 202. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 203. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 204. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 205. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 206. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 207. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 208. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 209. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 210. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 211. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 212. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 213. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 214. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 215. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 216. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 217. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 218. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 219. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 220. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 221. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 222. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 223. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 224. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 225. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 226. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 227. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 228. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 229. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 230. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 231. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 232. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 233. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 234. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 235. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 236. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 237. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 238. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 239. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 240. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 241. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 242. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 243. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 244. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 245. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 246. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 247. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 248. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 249. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 250. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 251. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 252. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 253. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 254. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 255. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 256. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 257. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 258. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 259. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 260. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 261. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 262. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 263. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 264. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 265. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 266. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 267. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 268. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 269. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 270. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 271. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 272. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 273. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 274. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 275. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 276. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 277. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 278. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 279. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 280. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 281. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 282. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 283. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 284. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 285. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 286. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 287. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 288. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 289. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 290. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 291. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 292. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 293. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 294. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 295. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 296. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 297. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 298. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 299. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 300. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 301. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 302. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 303. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 304. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 305. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 306. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 307. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 308. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 309. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 310. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 311. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 312. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 313. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 314. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 315. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 316. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 317. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 318. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 319. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 320. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 321. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 322. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 323. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 324. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 325. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 326. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 327. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 328. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 329. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 330. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 331. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 332. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 333. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 334. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 335. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 336. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 337. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 338. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 339. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 340. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 341. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 342. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 343. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 344. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 345. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 346. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 347. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 348. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 349. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 350. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 351. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 352. Skipping batch...\n",
            "Invalid loss value at Epoch 2, Batch 353. Skipping batch...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-07291758a044>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-1ea028f1244c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Take the last hidden state h_n as output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6. Evaluate Model"
      ],
      "metadata": {
        "id": "nX-RmFqC4sQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model's parameters\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_sequences.permute(1, 0, 2))  # Reshape inputs to (sequence_length, batch_size, input_size)\n",
        "    test_loss = criterion(outputs.squeeze(), test_targets)\n",
        "\n",
        "print(f'Test Loss: {test_loss.item():.4f}')"
      ],
      "metadata": {
        "id": "MqfhY9cY44r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7. Make Predictions"
      ],
      "metadata": {
        "id": "-a13JIaU4sZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = inputs.unsqueeze(1)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.squeeze().tolist())\n",
        "\n",
        "# Print some example predictions\n",
        "for i in range(10):\n",
        "    print('Expected:', test_data['rainfall_mm'].values[i], 'mm', 'Predicted:', predictions[i], 'mm')"
      ],
      "metadata": {
        "id": "ATM_RmBV47UX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}